{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_conll import init_parser\n",
    "nlp = init_parser('stanza', 'nl', parser_opts={'verbose': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('en')\n",
    "stanza_nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 21:23:50 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-11-09 21:23:50 INFO: Use device: cpu\n",
      "2020-11-09 21:23:50 INFO: Loading: tokenize\n",
      "2020-11-09 21:23:50 INFO: Loading: pos\n",
      "2020-11-09 21:23:51 INFO: Loading: lemma\n",
      "2020-11-09 21:23:51 INFO: Loading: depparse\n",
      "2020-11-09 21:23:53 INFO: Loading: sentiment\n",
      "2020-11-09 21:23:54 INFO: Loading: ner\n",
      "2020-11-09 21:23:55 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Barack PROPN nsubj:pass PERSON\n",
      "Obama Obama PROPN flat PERSON\n",
      "was be AUX aux:pass \n",
      "born bear VERB root \n",
      "in in ADP case \n",
      "Hawaii Hawaii PROPN obl GPE\n",
      ". . PUNCT punct \n",
      "He he PRON nsubj:pass \n",
      "was be AUX aux:pass \n",
      "elected elect VERB root \n",
      "president president PROPN xcomp \n",
      "in in ADP case \n",
      "2008 2008 NUM obl DATE\n",
      ". . PUNCT punct \n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from spacy_stanza import StanzaLanguage\n",
    "\n",
    "snlp = stanza.Pipeline(lang=\"en\")\n",
    "nlp = StanzaLanguage(snlp)\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "for token in doc:\n",
    "    print(token.)\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/airi/.cache/ig/models/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/airi/.cache/ig/models/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/airi/.cache/ig/models/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/airi/.cache/ig/models/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/airi/.cache/ig/models/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/airi/.cache/ig/models/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "annotator = stanfordnlp.Pipeline(\n",
    "            models_dir = str(pathlib.Path('~/.cache/ig/models/').expanduser().absolute()),\n",
    "            processors = \"tokenize,pos,lemma,depparse\",\n",
    "            lang = 'en',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "sent = \"Established in this Regulation subpart is the right to appeal to a revocation or certification.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tEstablished\testablish\tVERB\tVBN\tTense=Past|VerbForm=Part\t8\tcsubj\t_\t_\n",
      "2\tin\tin\tADP\tIN\t_\t5\tcase\t_\t_\n",
      "3\tthis\tthis\tDET\tDT\tNumber=Sing|PronType=Dem\t5\tdet\t_\t_\n",
      "4\tRegulation\tRegulation\tNOUN\tNN\tNumber=Sing\t5\tcompound\t_\t_\n",
      "5\tsubpart\tsubpart\tNOUN\tNN\tNumber=Sing\t1\tobl\t_\t_\n",
      "6\tis\tbe\tAUX\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t8\tcop\t_\t_\n",
      "7\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t8\tdet\t_\t_\n",
      "8\tright\tright\tNOUN\tNN\tNumber=Sing\t0\troot\t_\t_\n",
      "9\tto\tto\tPART\tTO\t_\t10\tmark\t_\t_\n",
      "10\tappeal\tappeal\tVERB\tVB\tVerbForm=Inf\t8\tacl\t_\t_\n",
      "11\tto\tto\tADP\tIN\t_\t13\tcase\t_\t_\n",
      "12\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t13\tdet\t_\t_\n",
      "13\trevocation\trevocation\tNOUN\tNN\tNumber=Sing\t10\tobl\t_\t_\n",
      "14\tor\tor\tCCONJ\tCC\t_\t15\tcc\t_\t_\n",
      "15\tcertification\tcertification\tNOUN\tNN\tNumber=Sing\t13\tconj\t_\t_\n",
      "16\t.\t.\tPUNCT\t.\t_\t8\tpunct\t_\t_\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "doc_response = annotator(sent)\n",
    "conll_string = doc_response.conll_file.conll_as_string()\n",
    "print(conll_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 1\n",
      "# text = Established in this Regulation subpart is the right to appeal to a revocation or certification.\n",
      "1\tEstablished\testablish\tVERB\tVBN\tVerbForm=part|Tense=past|Aspect=perf\t8\tcsubj\t_\t_\n",
      "2\tin\tin\tADP\tIN\t_\t5\tcase\t_\t_\n",
      "3\tthis\tthis\tDET\tDT\t_\t5\tdet\t_\t_\n",
      "4\tRegulation\tregulation\tNOUN\tNN\tNumber=sing\t5\tcompound\t_\t_\n",
      "5\tsubpart\tsubpart\tNOUN\tNN\tNumber=sing\t1\tobl\t_\t_\n",
      "6\tis\tbe\tAUX\tVBZ\tVerbForm=fin|Tense=pres|Number=sing|Person=three\t8\tcop\t_\t_\n",
      "7\tthe\tthe\tDET\tDT\t_\t8\tdet\t_\t_\n",
      "8\tright\tright\tNOUN\tNN\tNumber=sing\t0\troot\t_\t_\n",
      "9\tto\tto\tPART\tTO\tPartType=inf|VerbForm=inf\t10\tmark\t_\t_\n",
      "10\tappeal\tappeal\tVERB\tVB\tVerbForm=inf\t8\tacl\t_\t_\n",
      "11\tto\tto\tADP\tIN\t_\t13\tcase\t_\t_\n",
      "12\ta\ta\tDET\tDT\t_\t13\tdet\t_\t_\n",
      "13\trevocation\trevocation\tNOUN\tNN\tNumber=sing\t10\tobl\t_\t_\n",
      "14\tor\tor\tCCONJ\tCC\tConjType=comp\t15\tcc\t_\t_\n",
      "15\tcertification\tcertification\tNOUN\tNN\tNumber=sing\t13\tconj\t_\tSpaceAfter=No\n",
      "16\t.\t.\tPUNCT\t.\tPunctType=peri\t8\tpunct\t_\tSpaceAfter=No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sent)\n",
    "conll = doc._.conll_str\n",
    "print(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = init_parser(\"stanza\",\n",
    "                  \"en\",\n",
    "                  parser_opts={\"use_gpu\": True, \"verbose\": False},\n",
    "                  include_headers=True)\n",
    "# Parse a given string\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
